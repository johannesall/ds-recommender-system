{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative filtering - KNN with Surprise\n",
    "\n",
    "In this notebook you will learn about collaborative filtering and how to implement it with the surprise library. Collaborative filtering is a collective term for different recommendation algorithms based on user behavior. Those algorithm find users similar to each other based on their rating or clicking history. The interactions between users and items are stored in a so-called \"user-item interactions matrix\". These interactions can be explicit like actively giving ratings or implicit like click-data. In general there are two popular types of collaborative filtering approaches. The **user-based** filtering and the **item-based** filtering.\n",
    "\n",
    "**User-based** filtering algorithms predict ratings based on the ratings from similar (in terms of rating) users.</br>\n",
    "**Item-based** filtering algorithms predict ratings based on the ratings of similar (in terms of rating) items. Item-based models are especially used when you have way more users than items. Those models use average rating per item and not per user.\n",
    "\n",
    "A typical example of a problem collaborative filtering is trying to solve is the following: We have users, who rated specific items but a lot of item were not rated yet. We then try to predict the missing ratings denoted by red fields in this example of a user-item rating matrix.\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src = \"./images/UserItemRatingMatrix.png\">\n",
    "</p>\n",
    "<p align = \"center\">\n",
    "Fig.1 - User-Item-Rating Matrix - icons are from Vecteezy.com\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import KNNWithMeans, SVD\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our rating data. It contains the necessary `user_id`, `item_id` and the `rating` users gave to the fish items. Additionally it has some nice-to-have information about the fish items. There are 500 users with 300 rated fishes each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset from github may take some minutes -> coffee time :)\n",
    "df = pd.read_csv('data/user_item_ratings.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Surprise library we want to use does not work with pandas DataFrames but with Dataset objects. So we need to create a Dataset object from our DataFrame. We also need to define the possible ratings with the Reader class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines possible ratings\n",
    "reader = Reader(rating_scale=(1, 10))\n",
    "# Loads Pandas dataframe\n",
    "data = Dataset.load_from_df(df[[\"user_id\", \"item_id\", \"rating\"]], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to validate our models we need to split our data into a trainset, which we will use to train our models. And a testset to validate the ability of our models to predict on unseen data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and test set\n",
    "trainset, testset = train_test_split(data, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the modeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest-Neighbors\n",
    "One of the most common models for **collaborative filtering** is the **K-nearest neighbor algorithm (KNN)**. KNN is a **non-parametric**, **lazy learning** method. Lazy because it just stores the data-points without learning any kind of coefficient. To make predictions it calculates the \"distance\" between the target and every other instance, then it ranks the distances and returns the top **K** who are closest and therefore most similar to a given data point. Several ways exist to calculate the distances between the target and the other observations.\n",
    "\n",
    "As KNN's performance suffers from **curse of dimensionality** and e.g. **euclidean distance** is not optimal in high dimensions, **cosine similarity** is the most popular distance measure in terms of multi-dimensional data. Further description of the cosine similarity can be found in notebook 1. In this notebook we will use the [**KNNWithMeans**](https://surprise.readthedocs.io/en/stable/knn_inspired.html) algorithm implemented in the **surprise library**. This algorithm is directly derived from KNN but also takes the **mean ratings** of each user into account.\n",
    "\n",
    "For **user-based** the algorithm works as follows. First, we calculate the **similarity matrix** of the users. We use **cosine-similarity** here but other similarity measures can be used.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/UserSimilarityMatrix.png\">\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "Fig.1 - User-Similarity-Rating Matrix - icons are from Vecteezy.com\n",
    "</p>\n",
    "\n",
    "To then predict the rating for a certain fish by a certain user we simply take the sum of **k** (hyper parameter of the algorithm, here we use **k=2**) user ratings, with the highest similarity to our user, weighted by their similarity divided by the sum of used similarities.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/KNNExampleCalc.png\">\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "Fig.1 - User-Similarity-Rating Matrix - icons are from Vecteezy.com\n",
    "</p>\n",
    "\n",
    "Now let's see how the algorithm does on our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_options = {\n",
    "    \"name\": \"cosine\",   # Use Cosine-Similarity\n",
    "    \"user_based\": False,  # Compute  similarities between items\n",
    "}\n",
    "algo_knn = KNNWithMeans(sim_options=similarity_options, k=10, min_k=4)\n",
    "algo_knn.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict ratings for the testset\n",
    "predictions = algo_knn.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "print(f\"RMSE: {accuracy.rmse(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: `.test()` is a method that evaluates the entire test set and returns the predictions as a list of `Prediction` objects. Each object details the `user ID`, `item ID`, `actual rating`, and `estimated rating`. Additionally, the `.predict()` method is used for predicting the rating for a single user-item pair, returning a `Prediction` object that includes the estimated rating among other details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in predictions:\n",
    "    print(f\"user id:{element.uid}\", f\"item id:{element.iid}\", f\"estimated rating:{element.est}\", f\"real rating:{element.r_ui}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the top 10 recommendations for a specific user. Though there is no implementation of this in surprise the documentation provides a function `get_top_n` that returns the top-N recommendations, if we provide the predictions of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    \"\"\" Return the top-N recommendation for each user from a set of predictions.\n",
    "    \n",
    "    Args:\n",
    "    predictions(list of Prediction objects): The list of predictions, as\n",
    "        returned by the test method of an algorithm.\n",
    "    n(int): The number of recommendation to output for each user. Default\n",
    "        is 10.\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of\n",
    "        size n.\n",
    "    \"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    \n",
    "    for user_id, item_id, actual_rating, estimated_rating, _ in predictions:\n",
    "        top_n[user_id].append((item_id, estimated_rating))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for user_id, estimated_ratings in top_n.items():\n",
    "        estimated_ratings.sort(key=lambda x: x[1], reverse=True) # sort by rating estimation, descending. x[1] is the estimated rating. \n",
    "        top_n[user_id] = estimated_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we will get is a list of ten tuples (item_id, estimated_rating). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the top 10 recommendations for each user\n",
    "top_10 = get_top_n(predictions, n=10)\n",
    "top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the recommended items for a specific user\n",
    "user_id = 201   # user id\n",
    "# 10 best rated items for user id\n",
    "top_10[user_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make list of the top 10 item id's `top_iids`. And use it with the original fishes dataframe to get some characteristics of our recommended fishes. Apparently our user liked especially colorful fishes the most :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The top 10 recommendations for user_id 201 are:\n",
    "top_items_id_user_id = []\n",
    "for item_id, estimated_rating in top_10[user_id]:\n",
    "    print(f\"item id: {item_id}, estimated rating: {estimated_rating}\")\n",
    "    top_items_id_user_id.append(item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_items_id_user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the name of the recommended items\n",
    "recommended_fishes = df.set_index('item_id').loc[top_items_id_user_id][['name','fish_group','visual_effect']].drop_duplicates().copy()\n",
    "recommended_fishes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions for a new user\n",
    "Let's imagine we have a new user who has not rated any fish yet. This is a common issue called the **cold start problem**. For this user we could use the **most popular** fishes as a recommendation. This is a simple but effective way to start with. We can also ask the user to rate some items and then use the **user-based** or **item-based** collaborative filtering to make recommendations. One could directly use the trained model to make predictions for the new user or retrain the model with the new user's ratings.\n",
    "\n",
    "We will use the approch of asking the user to rate some items and then use the trained model `KNNWithMeans` to make recommendations. For this we will leverage the item-item similarity matrix *learnt* by the model.\n",
    "The item-item similarity matrix is used to predict the rating of a user for an item by taking the sum of the ratings of the **k** most similar items weighted by their similarity divided by the sum of the similarities. \n",
    "\n",
    "In the following we will show step by step how to make recommendations for a new user and then collect the steps in a function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1 - Collect Ratings from New User\n",
    "We will create a new user with user_id = 500 and collect ratings for some fishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new user ratings\n",
    "new_user_ratings = [\n",
    "    {\"user_id\": 500, \"item_id\": 1, \"rating\": 10},\n",
    "    {\"user_id\": 500, \"item_id\": 2, \"rating\": 9},\n",
    "    {\"user_id\": 500, \"item_id\": 3, \"rating\": 8},\n",
    "    {\"user_id\": 500, \"item_id\": 40, \"rating\": 7},\n",
    "    {\"user_id\": 500, \"item_id\": 50, \"rating\": 6},\n",
    "    {\"user_id\": 500, \"item_id\": 6, \"rating\": 5},\n",
    "    {\"user_id\": 500, \"item_id\": 390, \"rating\": 4},\n",
    "    {\"user_id\": 500, \"item_id\": 100, \"rating\": 3},\n",
    "    {\"user_id\": 500, \"item_id\": 9, \"rating\": 2},\n",
    "    {\"user_id\": 500, \"item_id\": 10, \"rating\": 1},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new user dataframe\n",
    "new_user_df = pd.DataFrame(new_user_ratings)\n",
    "new_user_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2 - Extract Similarity Matrix from trained `KNNWithMeans` Model\n",
    "The similarity matrix is stored in the `sim` attribute of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity matrix\n",
    "item_item_similarity_matrix = algo_knn.sim\n",
    "item_item_similarity_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity matrix is a numpy array with shape `(n_items, n_items)`. The similarity between item `i` and item `j` is stored in `sim[i, j]`. The similarity between item `i` and itself is stored in `sim[i, i]`. **Note** that `i` refers to the index of the item in the dataset\n",
    "and not the `item_id`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step3 - Select an Item and Convert it to the Index\n",
    "We will select an item  and convert it to the index in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an item_id\n",
    "item_id = 100\n",
    "item_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the inner id of the item\n",
    "item_inner_id = algo_knn.trainset.to_inner_iid(item_id)\n",
    "item_inner_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step4 - Get the Neighbors of the Item (Most Similar Items)\n",
    "We will get the neighbours of the item by using the `get_neighbors` method of the model. The method returns a list of inner indices of the most similar items to the selected item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the most similar items\n",
    "neighbors_inner_id = algo_knn.get_neighbors(item_inner_id, k=10)\n",
    "neighbors_inner_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step5 - Initialize Recommendations\n",
    "We will initialize the recommendations and the total similarity per item as empty dictionaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the recommendations and total similarity\n",
    "recommendations = defaultdict(float)\n",
    "total_similarity = defaultdict(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step6 - Calculate the Recommendations\n",
    "For each neighbour of the selected item we will calculate the recommendation by taking the sum of the ratings of the **k** most similar items weighted by their similarity divided by the sum of the similarities. We will store the recommendations in the recommendations dictionary and the total similarity per item in the total similarity dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for neighbor_inner_id in neighbors_inner_id:\n",
    "    neighbor_inner_raw_id = algo_knn.trainset.to_raw_iid(neighbor_inner_id)\n",
    "    # Prevent recommending items that the user has already rated\n",
    "    if neighbor_inner_raw_id not in new_user_df.item_id.values:\n",
    "        # Get the similarity score\n",
    "        similarity_score = item_item_similarity_matrix[item_inner_id, neighbor_inner_id]\n",
    "        ## Get the list of tuples of (inner id, rating) for the neighbor item\n",
    "        item_inner_id_ratings_list = algo_knn.trainset.ir[neighbor_inner_id]\n",
    "        ## Get only the ratings\n",
    "        ratings = [rating for (_, rating) in item_inner_id_ratings_list]\n",
    "        #print(ratings)\n",
    "        ## Calculate the total rating for the neighbor item\n",
    "        neighbor_total_rating = np.sum(ratings)\n",
    "        #print(neighbor_total_rating)\n",
    "        \n",
    "        # Accumulate weighted score and keep track of total similarity for normalization\n",
    "        recommendations[neighbor_inner_raw_id] = recommendations[neighbor_inner_raw_id] + similarity_score * (neighbor_total_rating)\n",
    "        total_similarity[neighbor_inner_raw_id] = total_similarity[neighbor_inner_raw_id] + similarity_score\n",
    "\n",
    "\n",
    "\n",
    "# Normalize the recommendations by both the total similarity and the number of ratings\n",
    "# This is to mitigate the bias towards items with a higher number of ratings\n",
    "for item_id, score in recommendations.items():\n",
    "    total_count_per_item = len(algo_knn.trainset.ir[item_id])\n",
    "    recommendations[item_id] = score / (total_similarity[item_id] * total_count_per_item)\n",
    "# Sort the recommendations by score\n",
    "sorted_recommendations = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a Function to Make Recommendations for a New \n",
    "We will collect the steps in a function `get_recommendations` that takes the new user's ratings, the trained model and the number of recommendations as input and returns the recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_recommendations(new_user_df, model=algo_knn, top_k=10):\n",
    "    \"\"\" Get recommendations for a new user based on the ratings provided. \n",
    "    Args:\n",
    "    new_user_df (pd.DataFrame): A dataframe containing the new user ratings.\n",
    "    model (surprise.prediction_algorithms.knns.KNNWithMeans): A trained KNNWithMeans model.\n",
    "    top_k (int): The number of recommendations to return.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the similarity matrix\n",
    "    item_item_similarity_matrix = model.sim\n",
    "    # Initialize the recommendations\n",
    "    recommendations = defaultdict(float)\n",
    "    total_similarity = defaultdict(float)\n",
    "    # Rated items\n",
    "    rated_items = set(new_user_df.item_id.values)\n",
    "\n",
    "    # Iterate over the new user ratings\n",
    "    for item_id in new_user_df.item_id.values:\n",
    "        # Get the inner id of the item\n",
    "        item_inner_id = model.trainset.to_inner_iid(item_id)\n",
    "        # Get the neighbors (the most similar items)\n",
    "        neighbors_inner_id = model.get_neighbors(item_inner_id, k=top_k)\n",
    "        # Iterate over the neighbors\n",
    "        for neighbor_inner_id in neighbors_inner_id:\n",
    "            # Get the raw id of the neighbor\n",
    "            neighbor_inner_raw_id = model.trainset.to_raw_iid(neighbor_inner_id)\n",
    "            # Prevent recommending items that the user has already rated\n",
    "            if neighbor_inner_raw_id not in rated_items:\n",
    "                # Get the similarity score between the item and the neighbor\n",
    "                similarity_score = item_item_similarity_matrix[item_inner_id, neighbor_inner_id]\n",
    "                # Get the list of tuples containing the ratings of the neighbor item\n",
    "                item_inner_id_ratings_list = model.trainset.ir[neighbor_inner_id]\n",
    "                ## Get only the ratings\n",
    "                ratings = [rating for (_, rating) in item_inner_id_ratings_list]\n",
    "                ## Get the total rating of the neighbor item\n",
    "                neighbor_total_rating = np.sum(ratings)\n",
    "                # Accumulate weighted score and keep track of total similarity for normalization\n",
    "                recommendations[neighbor_inner_raw_id] = recommendations[neighbor_inner_raw_id] + similarity_score * (neighbor_total_rating)\n",
    "                total_similarity[neighbor_inner_raw_id] = total_similarity[neighbor_inner_raw_id] + similarity_score\n",
    "                \n",
    "    # Normalize the recommendations by both the total similarity and the number of ratings\n",
    "    # This is to mitigate the bias towards items with a higher number of ratings\n",
    "    for item_id, score in recommendations.items():\n",
    "        total_count_per_item = len(algo_knn.trainset.ir[item_id])\n",
    "        recommendations[item_id] = score / (total_similarity[item_id] * total_count_per_item)\n",
    "    # Sort and return the recommendations\n",
    "    sorted_recommendations = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_recommendations[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_items = get_recommendations(new_user_df, model=algo_knn, top_k=10)\n",
    "print(\"Recommended Items:\", recommended_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we learned how to use collaborative filtering to make recommendations based on the idea of similarity: \n",
    "- **user-based filtering**: *users who are similar to you also liked ...* \n",
    "- **item-based**: *because you watched/bought ... you may also like ...*\n",
    "\n",
    "\n",
    "We used the Scikit-Surprise library to train a KNNWithMeans model on a custom dataset of user ratings for fish items. We then used the model to make recommendations for a new user by leveraging the item-item similarity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Surprise Library](https://surprise.readthedocs.io/en/stable/index.html)\n",
    "- [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "- [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)\n",
    "- [Collaborative Filtering](https://en.wikipedia.org/wiki/Collaborative_filtering)\n",
    "- [Netflix Prize](https://en.wikipedia.org/wiki/Netflix_Prize)\n",
    "- [Simon Funk](https://sifter.org/simon/journal/20061211.html)\n",
    "- [Cold Start Problem](https://en.wikipedia.org/wiki/Cold_start_(recommender_systems))\n",
    "- [Implicit Recommender Systems](https://andbloch.github.io/An-Overview-of-Collaborative-Filtering-Algorithms/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surprise_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
